---
title: "Forecasting Gainesville Electricity Consumption"
subtitle: "Use data collected by Gainesville Regional Utilities to model and forecast 2 year ahead electricity demand"
author: "Sean Cannon"
date: "`r format(Sys.Date(),'%B %d, %Y')`"
format:
  html:
    self-contained: true
    code-fold: true
    code-summary: "Show the code"
editor: visual
---

```{r setup, message=FALSE, warning=FALSE}
# required packages
library(tidyverse)
library(janitor)
library(lubridate)
library(fpp3)
library(kableExtra)
library(sf)
library(cowplot)
library(leaflet)

# optional packages
#library(tidylog)

# lat/long field 
options(scipen = 999, digits=8)
```

## Introduction

Accurate forecasting better helps utility companies plan for electricity consumption and minimize excess production. This project uses forecasting and time series analysis methods to create forecasts for two year-ahead monthly electricity demand for the city of Gainesville, Florida.

This analysis uses the `fpp3` package and implements time-series tools from the [third edition](https://otexts.com/fpp3/.) of Forecasting: Principles and Practice.^\[1\]^

## Gathering Information

Gainesville Regional Utilities (GRU) is a multi-service utility company owned by the City of Gainesville, FL that provides electricity, wastewater, and natural gas to city residents. GRU posts customer [electricity consumption data](https://data.cityofgainesville.org/Utilities/GRU-Customer-Electric-Consumption-2012-2022/ba7j-nifw/about_data) from 2012 - 2022 publicly on their open data portal.^\[2\]^

Steps taken in the data cleaning process:

-   Read in only 2012 - 2022 data. Year 2023 is incomplete.
-   Remove the handful of records in ALACHUA and NEWBERRY.
-   Remove the single record missing service address.
-   Cast character date fields as date data type values.

```{r, message=FALSE, warning=FALSE, include=TRUE}
# read in the Gainesville electric consumption data for years 2012 - 2022
gru_data <- readr::read_csv('GRU_Customer_Electric_Consumption_2012-2022_20250327.csv') %>%
  janitor::clean_names() %>%
  filter(year %in% 2012:2022)

# do some data cleaning
elec_cons_data <- gru_data %>%
  # remove ALACHUA and NEWBERRY records 
  filter(service_city == "GAINESVILLE") %>%
  # remove the single record missing address
  filter(!is.na(service_address)) %>%
  # remove records with kwh consumption > 5000000
  # this filter removes a single record - the top kwh consumption value in the dataset,
  # which was visually inspected as bad data
  filter(kwh_consumption < 5000000) %>%
  # get date variable in ISO date format and create a new year/month variable
  mutate(date2 = as.Date(date, format = "%m/%d/%Y"),
         month2 = zoo::as.yearmon(date2, "%Y %m")) %>%
  # drop state, service_city, and old date vars
  select(!c('state','service_city','year','month','date')) %>%
  rename(date = date2,
         month = month2) %>%
  # reorder columns and sort by month ascending
  select(month, date, service_address, kwh_consumption, latitude, longitude, geo) %>%
  arrange(month)

# look at the count of missing values for each field
elec_cons_data %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(),
               names_to = "var",
               values_to = "value") %>%
  mutate(misspct = value / nrow(elec_cons_data)) %>%
  mutate(across(value, scales::label_comma())) %>%
  mutate(across(misspct, scales::label_percent())) %>%
  kbl(col.names = c("Variable", "Missing Count", "Missing Percent"), align = "ccc",
      table.attr = 'data-quarto-disable-processing="true"') %>% 
  kable_styling(bootstrap_options = c("striped", "hover",
                                      "condensed", "responsive"), full_width = F)
```

#### Missing Values

Overall, the key fields used in this analysis are well populated. The `latitude` and `longitude` fields are systemically missing values over each month/year. The geographic point level `geo` field appears to not have been populated until August of 2021.

```{r, message=FALSE, warning=FALSE}
# look at the count of unique values for each field
elec_cons_data %>%
  summarise(across(everything(), ~ n_distinct(., na.rm = TRUE))) %>%
  pivot_longer(cols = everything(),
               names_to = "var",
               values_to = "value") %>%
  mutate(across(value, scales::label_comma())) %>%
  kbl(col.names = c("Variable", "Unique Values"), align = "cc",
      table.attr = 'data-quarto-disable-processing="true"') %>% 
  kable_styling(bootstrap_options = c("striped", "hover",
                                      "condensed", "responsive"), full_width = F)
```

#### Duplicates

Date is unique within each month, which is important for aggregation later. There are fewer distinct values for `latitude` and `longitude` because some service addresses are systemically lacking coordinates. The point level `geo` field is incongruent with the `latitude` and `longitude` field and only contains a handful of unique values, so it won't be used in this analysis.

```{r, warning=FALSE, include=FALSE}
#| column: page

# most frequently occuring value in geo field
leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(lng=-82.325024, lat=29.651958, popup = "Main Street and University Avenue")
```

```{r, warning=FALSE, message=FALSE}
duplicates <- elec_cons_data %>%
   group_by(date, service_address) %>%
   filter(n() > 1) %>%
   ungroup()

# if there are multiple entries in a particular date, sum them
# other options - keep the largest, keep the smallest, etc.
# this method we keep all information, and we also dont know what is correct vs incorrect
elec_data_dedup <- elec_cons_data %>%
  group_by(date, service_address) %>%
  summarize(rollup_kwh = sum(kwh_consumption)) %>%
  mutate(month = zoo::as.yearmon(date, "%Y %m"),
         month_name = month(month, label = TRUE, abbr = FALSE),
         year = year(month)) %>%
  arrange(month)

# boot barn on archer rd is part of the duplicates
elec_cons_data %>%
  filter(service_address=='3638 SW ARCHER RD') %>%
  select(month, date, service_address, kwh_consumption) %>%
  head(6)
```

Some `service_address` values have duplicate records for `kwh_consumption` within each date. Typically, it is one large value and one (or several) smaller values together. Lacking further information, it's difficult to tell whether the smaller values are legitimate electricity consumption for that address. This analysis takes the sum instead of dropping the duplicates to preserve all available information.

```{r}
# take the sum of each month to get total kwh consumption for archer rd boot barn
elec_data_dedup %>%
  filter(service_address=='3638 SW ARCHER RD') %>%
  select(month, date, service_address, rollup_kwh) %>%
  head(3)
```

This operation removes duplicate values by `month` and `service_address`. Now, `rollup_kwh` variable has been created as the sum of all values within each `month` and `service_address` group.

## Exploratory Data Analysis

```{r, message=FALSE, warning=FALSE}
# Summary statistics for ungrouped data
# cat("Summary statistics for rollup_kwh variable:\n")
# cat("\nMean of rollup_kwh:", mean(elec_data_dedup$rollup_kwh), "\n")
# cat("Median of rollup_kwh:", median(elec_data_dedup$rollup_kwh), "\n")
# cat("Minimum value of rollup_kwh:", min(elec_data_dedup$rollup_kwh), "\n")
# cat("Maximum value of rollup_kwh:", max(elec_data_dedup$rollup_kwh), "\n")
# cat("Standard deviation of rollup_kwh:", sd(elec_data_dedup$rollup_kwh), "\n")
# cat("Quantiles of rollup_kwh:")
# quantile(elec_data_dedup$rollup_kwh)
```

| Statistic          | Value (kWh Consumption) |
|--------------------|-------------------------|
| Mean               | 1,585                   |
| Minimum            | 0                       |
| 25%                | 440                     |
| Median             | 717                     |
| 75%                | 1,108                   |
| Maximum            | 8,009,119               |
| Standard Deviation | 19,195                  |

: Summary Statistics for GRU Electricity Consumption Dataset 2012-2022

Comparing the mean and the median values show that the distribution of values for electricity consumption are highly right-skewed. The 25th and 75th percentiles show that most of the values fall within a more narrow range, but there are some extreme positive outliers. The maximum of kWh consumption is around 8 million kWh (which equates to 8 gigawatt hours) in one month. The standard deviation is very large, showing that the data are not tightly clustered around the mean. The minimum of zero is reasonable, however, as there are no negative values for electricity consumption in the distribution and it's plausible that some properties on the grid may not consume any electricity in a given month.

```{r}
# look at the summary stats by month
elec_data_dedup %>%
  group_by(month_name) %>%
  summarise(
    mean_kwh_by_month = mean(rollup_kwh),
    med_kwh_by_month = median(rollup_kwh),
    sd_kwh_by_month = sd(rollup_kwh)) %>%
  mutate(across(mean_kwh_by_month:sd_kwh_by_month, scales::label_comma())) %>%
  kbl(col.names = c("Month", "Mean","Median","Standard Deviation"), align = "cccc",
      table.attr = 'data-quarto-disable-processing="true"') %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F)
```

```{r, warning=FALSE}
# shape of the curve changes in the summer months May - Sept
elec_data_dedup %>%
  ggplot(aes(x = rollup_kwh)) +
  geom_histogram(binwidth = 100, fill = "goldenrod2", color = "black", alpha = 0.7) +
  scale_x_continuous(limits = c(0,5000),
                     breaks = seq(0,5000,by=1000),
                     labels = c('','1k','2k','3k','4k','5k')) +
  scale_y_continuous(labels = scales::unit_format(unit = "k", scale = 1e-3)) +
  labs(title = "Distribution of Gainesville Property-Level kWh Consumption by Month",
       x = "kWh Consumption",
       y = "Count",
       caption = "This chart has been limited to observations with less than 5000 kWh consumption values.") +
  facet_wrap(~ month_name) +
  theme_minimal()
```

::: callout-note
The distribution has a strong right-skew, causing considerable differences between the mean and median. There is clear seasonality when viewing the distributions by month as the shape of the distributions shift during the hot summer months when there is increased electricity demand to run air conditioning.
:::

#### Visualize the time series

```{r}
# aggregate the data by month and convert to gWh
elec_data_agg <- elec_data_dedup %>%
  group_by(month) %>%
  summarize(mthly_kwh_sum = sum(rollup_kwh)) %>%
  mutate(mthly_gwh_sum = mthly_kwh_sum / 1000000,
         newyrmon = tsibble::yearmonth(month)) %>%
  select(!month) %>%
  as_tsibble(index = newyrmon) %>%
  rename(month = newyrmon) %>%
  select(month, mthly_kwh_sum, mthly_gwh_sum) %>%
  arrange(month)

elec_data_agg %>% 
  autoplot(mthly_gwh_sum, size=.75,color="#04052E") +
  geom_point() +
  labs(title="Gainesville Monthly Electricity Consumption (gWh)",
       x="Month",
       y='gWh') +
  theme_minimal()
```

::: callout-note
Electricity consumption is highly seasonal, exhibiting peaks in the summer months and troughs in the winter months. Let's explore the seasonality more.
:::

#### Seasonality

```{r, warning=FALSE, message=FALSE}

# elec_data_agg %>%
#   gg_subseries(mthly_gwh_sum) +
#   labs(title = "",
#        x="",
#        y='') +
#   theme_minimal() +
#   theme(axis.text.x=element_text(angle=-90))

elec_data_agg %>%
  gg_season(mthly_gwh_sum) +
  labs(title = "Seasonal Pattern in Gainesville Electricity Consumption",
       x="Month",
       y='gWh') +
  theme_minimal()
```

Looking at the seasonality chart, June to October are the months with the highest electricity consumption in Gainesville. As a former Florida resident, these are the hottest months out of the year! It makes sense that these months correspond with the highest demand for electricity.

#### Autocorrelation

```{r}
elec_data_agg %>%
  ACF(mthly_gwh_sum, lag_max = 24) %>%
  autoplot() + 
  labs(title="Autocorrelation Function for Electricity Consumption") +
  theme_minimal()
```

Exploring autocorrelation shows to what degree variables are correlated with their past values. This autocorrelation chart displays a clear seasonal pattern – electricity consumption in the current period is strongly correlated with lags 12 and 24 (the same month in previous years). Similarly, electricity consumption in the current period is strongly negatively correlated with lags 6 and 18 (the corresponding month in the opposite season).

## Decomposition

```{r, warning=FALSE}
#| column: page
#| layout-nrow: 1

dcmp <- elec_data_agg %>%
  model(stl = STL(mthly_gwh_sum ~
                    trend(window = 9) +
                    season(window = "periodic"),
                  robust = TRUE))

components(dcmp) %>%
  autoplot() +
  labs(title = 'STL Decomposition for Monthly gWh Consumption',
       x = "Month") +
  theme_minimal()

components(dcmp) %>%
  as_tsibble() %>%
  autoplot(mthly_gwh_sum, color="darkgrey") +
  geom_line(aes(y=trend), size=.75,color = "#ff006d") +
  labs(title = "Trend in Electricity Consumption",
       x="Month",
       y='gWh') +
  theme_minimal()
```

Let's decompose the time series into seasonal, trend, and residual components using an STL decomposition. The decomposition specified above has a relatively small trend component, constant seasonal effect, and is robust to outliers. The trend shows a downward drift from 2012 to 2015, followed by a spike from around 2016 to 2018. This is potentially due to weather anomalies or structural changes in electricity consumption during these time periods. This is an interesting finding, however, as I expected a constant positive trend as Gainesville has undergone substantial development over the last decade or two. There is a regular seasonal pattern in the data, and the remainders show that most values hover near zero (with a few inconsistent spikes).

```{r}
# check for a regular unit root
elec_data_agg %>%
  features(mthly_gwh_sum, unitroot_ndiffs)

# check for a seasonal unit root
elec_data_agg %>%
  features(mthly_gwh_sum, unitroot_nsdiffs)
```

::: callout-note
The electricity consumption time series does not have a typical unit root and therefore does not require trend differencing. However, it does have a seasonal unit root. To make good forecasts, seasonal differencing will be applied to account for changes in seasonal patterns over time (this is automatically done by the `ARIMA()` model function).
:::

## Fit Models

```{r}
# split into train test (9 years train, 2 years test)
train <- elec_data_agg %>% filter_index(~ "2020-12")
test <- elec_data_agg %>% filter_index("2021-01" ~ .)

# fit snaive and sarima models
fit_models <- train %>%
  model(SNaive = SNAIVE(mthly_gwh_sum),
        SARIMA = ARIMA(mthly_gwh_sum))

# view details of the two models
report(fit_models[1])
report(fit_models[2])
```

Since these data are highly seasonal, the benchmark model will be the seasonal naïve method. This forecast style takes the previous values for each month and copies them a year ahead. This is a very simple method but is an appropriate benchmark to evaluate more complicated models based on the electricity consumption time series being highly seasonal.

<center>

**Seasonal Naïve Model**

$\hat{y}_{t+h} = y_{t+h - mk}​$

</center>

The more complicated model used is a seasonal ARIMA model. This is an ARIMA model that has additional seasonal terms to account for seasonal periods in the data. The model that has been auto specified based on the electricity consumption time series has the specification: **ARIMA(1,0,1)(2,1,0)\[12\]**. Breaking down the terms, it is has one autoregressive lag (AR(1)), one moving average (MA(1)), two seasonal AR terms for lags 12 and 24, and one difference term. The difference term is required due to the seasonal differencing mentioned in the note above. Written formally (using backshift notation):

<center>

**SARIMA Model**

$(1 - \phi_1 B)(1 - \Phi_1 B^{12} - \Phi_2 B^{24})(1 - B^{12}) y_t = (1 + \theta_1 B)\varepsilon_t$

</center>

The hope is that the seasonal ARIMA model is specified well enough that it will outperform the simple benchmark. Let's see how well these models forecast electricity consumption in Gainesville.

## Forecast and Evaluate

```{r}

# forecast for the test period
elec_fc <- forecast(fit_models, new_data = test, level = 95)

# plot the forecasts with confidence intervals
elec_fc %>%
  autoplot(train, size=.75, level = c(95, 80)) +
  labs(title="Forecasts: Seasonal Naïve and SARIMA Models",
       x="Month",
       y='gWh') +
  theme_minimal()

# in-sample accuracy
accuracy(fit_models)

# out-of-sample accuracy
accuracy(elec_fc, test)
```

The SARIMA model beats the benchmark seasonal naïve method and is clearly the stronger forecasting model. It fits the training data with low error rates and performs better on the test data, and it consistently outperforms the benchmark in all major metrics of forecast error. Notably, the SARIMA model has some correlation in the residuals as indicated by the ACF1, meaning that this model could be improved using additional adjustments. Overall, the SARIMA model has solid performance predicting seasonal electricity demand in Gainesville, FL!

## References

^\[1\]^ Hyndman, Rob J. “Forecasting: Principles and Practice (3rd Ed).” OTexts. Accessed April 10, 2022. <https://otexts.com/fpp3/.>

^\[2\]^ [Gainesville Regional Utilities Electricity Consumption Data](https://data.cityofgainesville.org/Utilities/GRU-Customer-Electric-Consumption-2012-2022/ba7j-nifw/about_data)
